{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import jaccard\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jackard(feature):\n",
    "    idx = feature.shape[0]\n",
    "    dist_mean = np.ones((idx))\n",
    "    dist_median = np.ones((idx))\n",
    "    dist_min = np.ones((idx))\n",
    "    dist_max = np.ones((idx))\n",
    "    for i in range(idx):    \n",
    "        dist_mat = np.array([jaccard(feature[i], feature[j]) for j in range(idx) if j != i])\n",
    "        dist_mat[np.isnan(dist_mat)] = 1.0\n",
    "        dist_mean[i] = dist_mat.mean()\n",
    "        dist_median[i] = np.median(dist_mat)\n",
    "        dist_min[i] = dist_mat.min()\n",
    "        dist_max[i] = dist_mat.max()\n",
    "    return dist_mean, dist_median, dist_min, dist_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>group_id</th>\n",
       "      <th>title</th>\n",
       "      <th>lemmatized_titles_with_nltk_and_maru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6710</td>\n",
       "      <td>11691</td>\n",
       "      <td>130</td>\n",
       "      <td>КАК ПРОПИСАТЬ АДМИНКУ В КС 1.6 СЕБЕ ИЛИ ДРУГУ ...</td>\n",
       "      <td>как прописать админка в кс 1.6 себя или друг y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4030</td>\n",
       "      <td>11692</td>\n",
       "      <td>130</td>\n",
       "      <td>Скачать: SGL-RP доработка | Слив мода [MySQL] ...</td>\n",
       "      <td>скачать sgl-rp доработка слива мода mysql rp r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5561</td>\n",
       "      <td>11693</td>\n",
       "      <td>130</td>\n",
       "      <td>Как прописать админку в кс 1.6 - Counter-Strik...</td>\n",
       "      <td>как прописать админка кс 1.6 counter-strike ка...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4055</td>\n",
       "      <td>11694</td>\n",
       "      <td>130</td>\n",
       "      <td>Как прописать простую админку в кс 1 6</td>\n",
       "      <td>как прописать простой админка кс 1 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4247</td>\n",
       "      <td>11695</td>\n",
       "      <td>130</td>\n",
       "      <td>Подбор админов для сервера по КОД_4 [Архив]  ...</td>\n",
       "      <td>подбор админ сервер код_4 архив форум ozone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id  pair_id  group_id  \\\n",
       "0    6710    11691       130   \n",
       "1    4030    11692       130   \n",
       "2    5561    11693       130   \n",
       "3    4055    11694       130   \n",
       "4    4247    11695       130   \n",
       "\n",
       "                                               title  \\\n",
       "0  КАК ПРОПИСАТЬ АДМИНКУ В КС 1.6 СЕБЕ ИЛИ ДРУГУ ...   \n",
       "1  Скачать: SGL-RP доработка | Слив мода [MySQL] ...   \n",
       "2  Как прописать админку в кс 1.6 - Counter-Strik...   \n",
       "3             Как прописать простую админку в кс 1 6   \n",
       "4   Подбор админов для сервера по КОД_4 [Архив]  ...   \n",
       "\n",
       "                lemmatized_titles_with_nltk_and_maru  \n",
       "0  как прописать админка в кс 1.6 себя или друг y...  \n",
       "1  скачать sgl-rp доработка слива мода mysql rp r...  \n",
       "2  как прописать админка кс 1.6 counter-strike ка...  \n",
       "3               как прописать простой админка кс 1 6  \n",
       "4        подбор админ сервер код_4 архив форум ozone  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('titles_and_lemmatized_titles_with_nltk_and_maru_train.csv', encoding='utf-8')\n",
    "train_data.fillna('', inplace=True)\n",
    "test_data = pd.read_csv('titles_and_lemmatized_titles_with_nltk_and_maru_test.csv', encoding='utf-8')\n",
    "test_data.fillna('', inplace=True)\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data[train_data.doc_id == 2455]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 12)\n"
     ]
    }
   ],
   "source": [
    "last_group = train_data.group_id.iloc[-1]\n",
    "\n",
    "f1_mean = np.array([])\n",
    "f1_med = np.array([])\n",
    "f1_min = np.array([])\n",
    "f1_max = np.array([])\n",
    "f2_mean = np.array([])\n",
    "f2_med = np.array([])\n",
    "f2_min = np.array([])\n",
    "f2_max = np.array([])\n",
    "f3_mean = np.array([])\n",
    "f3_med = np.array([])\n",
    "f3_min = np.array([])\n",
    "f3_max = np.array([])\n",
    "\n",
    "f1_lem_mean = np.array([])\n",
    "f1_lem_med = np.array([])\n",
    "f1_lem_min = np.array([])\n",
    "f1_lem_max = np.array([])\n",
    "f2_lem_mean = np.array([])\n",
    "f2_lem_med = np.array([])\n",
    "f2_lem_min = np.array([])\n",
    "f2_lem_max = np.array([])\n",
    "f3_lem_mean = np.array([])\n",
    "f3_lem_med = np.array([])\n",
    "f3_lem_min = np.array([])\n",
    "f3_lem_max = np.array([])\n",
    "\n",
    "for group in range(1, last_group + 1):\n",
    "    mask_gr = train_data.group_id == group\n",
    "    t_gr = train_data.title[mask_gr]\n",
    "    t_lem_gr = train_data.lemmatized_titles_with_nltk_and_maru[mask_gr]\n",
    "\n",
    "    cv_word_1_1 = CountVectorizer()\n",
    "    cv_word_1_1.fit(t_gr)\n",
    "    feat_1 = cv_word_1_1.transform(t_gr).toarray()\n",
    "    mean_1, med_1, min_1, max_1 = compute_jackard(feat_1)\n",
    "    \n",
    "    cv_lem_word_1_1 = CountVectorizer()\n",
    "    cv_lem_word_1_1.fit(t_lem_gr)\n",
    "    feat_lem_1 = cv_lem_word_1_1.transform(t_lem_gr).toarray()\n",
    "    mean_lem_1, med_lem_1, min_lem_1, max_lem_1 = compute_jackard(feat_lem_1)\n",
    "    \n",
    "    cv_word_2_2 = CountVectorizer(analyzer='word', ngram_range=(2,2))\n",
    "    cv_word_2_2.fit(t_gr)\n",
    "    feat_2 = cv_word_2_2.transform(t_gr).toarray()\n",
    "    mean_2, med_2, min_2, max_2 = compute_jackard(feat_2)\n",
    "    \n",
    "    cv_lem_word_2_2 = CountVectorizer()\n",
    "    cv_lem_word_2_2.fit(t_lem_gr)\n",
    "    feat_lem_2 = cv_lem_word_2_2.transform(t_lem_gr).toarray()\n",
    "    mean_lem_2, med_lem_2, min_lem_2, max_lem_2 = compute_jackard(feat_lem_2)\n",
    "    \n",
    "    cv_char_3_4 = CountVectorizer(analyzer='char', ngram_range=(3,4))\n",
    "    cv_char_3_4.fit(t_gr)\n",
    "    feat_3 = cv_char_3_4.transform(t_gr).toarray()\n",
    "    mean_3, med_3, min_3, max_3 = compute_jackard(feat_3)\n",
    "        \n",
    "    cv_lem_char_3_4 = CountVectorizer()\n",
    "    cv_lem_char_3_4.fit(t_lem_gr)\n",
    "    feat_lem_3 = cv_lem_char_3_4.transform(t_lem_gr).toarray()\n",
    "    mean_lem_3, med_lem_3, min_lem_3, max_lem_3 = compute_jackard(feat_lem_3)\n",
    "    \n",
    "    f1_mean = np.concatenate((f1_mean, mean_1))\n",
    "    f1_med = np.concatenate((f1_med, med_1))\n",
    "    f1_min = np.concatenate((f1_min, min_1))\n",
    "    f1_max = np.concatenate((f1_max, max_1))\n",
    "    f2_mean = np.concatenate((f2_mean, mean_2))\n",
    "    f2_med = np.concatenate((f2_med, med_2))\n",
    "    f2_min = np.concatenate((f2_min, min_2))\n",
    "    f2_max = np.concatenate((f2_max, max_2))\n",
    "    f3_mean = np.concatenate((f3_mean, mean_3))\n",
    "    f3_med = np.concatenate((f3_med, med_3))\n",
    "    f3_min = np.concatenate((f3_min, min_3))\n",
    "    f3_max = np.concatenate((f3_max, max_3))\n",
    "    \n",
    "    f1_lem_mean = np.concatenate((f1_lem_mean, mean_lem_1))\n",
    "    f1_lem_med = np.concatenate((f1_lem_med, med_lem_1))\n",
    "    f1_lem_min = np.concatenate((f1_lem_min, min_lem_1))\n",
    "    f1_lem_max = np.concatenate((f1_lem_max, max_lem_1))\n",
    "    f2_lem_mean = np.concatenate((f2_lem_mean, mean_lem_2))\n",
    "    f2_lem_med = np.concatenate((f2_lem_med, med_lem_2))\n",
    "    f2_lem_min = np.concatenate((f2_lem_min, min_lem_2))\n",
    "    f2_lem_max = np.concatenate((f2_lem_max, max_lem_2))\n",
    "    f3_lem_mean = np.concatenate((f3_lem_mean, mean_lem_3))\n",
    "    f3_lem_med = np.concatenate((f3_lem_med, med_lem_3))\n",
    "    f3_lem_min = np.concatenate((f3_lem_min, min_lem_3))\n",
    "    f3_lem_max = np.concatenate((f3_lem_max, max_lem_3))\n",
    "\n",
    "feat_to_df = {'mean_word_1_1': f1_mean, 'median_word_1_1': f1_med,\n",
    "              'min_word_1_1': f1_min, 'max_word_1_1': f1_max,\n",
    "              'mean_word_2_2': f2_mean, 'median_word_2_2': f2_med,\n",
    "              'min_word_2_2': f2_min, 'max_word_2_2': f2_max,\n",
    "              'mean_char_3_4': f3_mean, 'median_char_3_4': f3_med,\n",
    "              'min_char_3_4': f3_min, 'max_char_3_4': f3_max}\n",
    "df_features = pd.DataFrame(feat_to_df)\n",
    "\n",
    "feat_to_df = {'mean_word_1_1': f1_lem_mean, 'median_word_1_1': f1_lem_med,\n",
    "              'min_word_1_1': f1_lem_min, 'max_word_1_1': f1_lem_max,\n",
    "              'mean_word_2_2': f2_lem_mean, 'median_word_2_2': f2_lem_med,\n",
    "              'min_word_2_2': f2_lem_min, 'max_word_2_2': f2_lem_max,\n",
    "              'mean_char_3_4': f3_lem_mean, 'median_char_3_4': f3_lem_med,\n",
    "              'min_char_3_4': f3_lem_min, 'max_char_3_4': f3_lem_max}\n",
    "df_lem_features = pd.DataFrame(feat_to_df)\n",
    "\n",
    "print(df_lem_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_word_1_1</th>\n",
       "      <th>median_word_1_1</th>\n",
       "      <th>min_word_1_1</th>\n",
       "      <th>max_word_1_1</th>\n",
       "      <th>mean_word_2_2</th>\n",
       "      <th>median_word_2_2</th>\n",
       "      <th>min_word_2_2</th>\n",
       "      <th>max_word_2_2</th>\n",
       "      <th>mean_char_3_4</th>\n",
       "      <th>median_char_3_4</th>\n",
       "      <th>min_char_3_4</th>\n",
       "      <th>max_char_3_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.902380</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902380</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902380</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.941398</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.941398</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.941398</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.970934</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.970934</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.970934</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.930193</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.930193</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.930193</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_word_1_1  median_word_1_1  min_word_1_1  max_word_1_1  mean_word_2_2  \\\n",
       "0       0.902380         0.937500      0.142857           1.0       0.902380   \n",
       "1       0.941398         0.964286      0.533333           1.0       0.941398   \n",
       "2       0.970934         1.000000      0.850000           1.0       0.970934   \n",
       "3       1.000000         1.000000      1.000000           1.0       1.000000   \n",
       "4       0.930193         1.000000      0.500000           1.0       0.930193   \n",
       "\n",
       "   median_word_2_2  min_word_2_2  max_word_2_2  mean_char_3_4  \\\n",
       "0         0.937500      0.142857           1.0       0.902380   \n",
       "1         0.964286      0.533333           1.0       0.941398   \n",
       "2         1.000000      0.850000           1.0       0.970934   \n",
       "3         1.000000      1.000000           1.0       1.000000   \n",
       "4         1.000000      0.500000           1.0       0.930193   \n",
       "\n",
       "   median_char_3_4  min_char_3_4  max_char_3_4  \n",
       "0         0.937500      0.142857           1.0  \n",
       "1         0.964286      0.533333           1.0  \n",
       "2         1.000000      0.850000           1.0  \n",
       "3         1.000000      1.000000           1.0  \n",
       "4         1.000000      0.500000           1.0  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lem_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean_word_1_1      0\n",
       "median_word_1_1    0\n",
       "min_word_1_1       0\n",
       "max_word_1_1       0\n",
       "mean_word_2_2      0\n",
       "median_word_2_2    0\n",
       "min_word_2_2       0\n",
       "max_word_2_2       0\n",
       "mean_char_3_4      0\n",
       "median_char_3_4    0\n",
       "min_char_3_4       0\n",
       "max_char_3_4       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jackard_titles_train_not_nan.csv', mode='w', encoding='utf-8') as f_csv:\n",
    "#     df_features.fillna(1.0, inplace=True)\n",
    "    df_features.to_csv(f_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized = preprocessing.normalize(df_features)\n",
    "# normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('jackard_titles_train_not_nan_normalized.csv', mode='w', encoding='utf-8') as f_csv:\n",
    "# #     df_features.fillna(1.0, inplace=True)\n",
    "#     df_features.to_csv(f_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean_word_1_1      0\n",
       "median_word_1_1    0\n",
       "min_word_1_1       0\n",
       "max_word_1_1       0\n",
       "mean_word_2_2      0\n",
       "median_word_2_2    0\n",
       "min_word_2_2       0\n",
       "max_word_2_2       0\n",
       "mean_char_3_4      0\n",
       "median_char_3_4    0\n",
       "min_char_3_4       0\n",
       "max_char_3_4       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lem_features.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jackard_titles_lemmatization_train_not_nan.csv', mode='w', encoding='utf-8') as f_csv:\n",
    "#     df_lem_features.fillna(1.0, inplace=True)\n",
    "    df_lem_features.to_csv(f_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized__lem = preprocessing.normalize(df_lem_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lem_features[df_lem_features.mean_word_2_2.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 12)\n"
     ]
    }
   ],
   "source": [
    "first_group = test_data.group_id.iloc[0]\n",
    "last_group = test_data.group_id.iloc[-1]\n",
    "\n",
    "f1_mean = np.array([])\n",
    "f1_med = np.array([])\n",
    "f1_min = np.array([])\n",
    "f1_max = np.array([])\n",
    "f2_mean = np.array([])\n",
    "f2_med = np.array([])\n",
    "f2_min = np.array([])\n",
    "f2_max = np.array([])\n",
    "f3_mean = np.array([])\n",
    "f3_med = np.array([])\n",
    "f3_min = np.array([])\n",
    "f3_max = np.array([])\n",
    "\n",
    "f1_lem_mean = np.array([])\n",
    "f1_lem_med = np.array([])\n",
    "f1_lem_min = np.array([])\n",
    "f1_lem_max = np.array([])\n",
    "f2_lem_mean = np.array([])\n",
    "f2_lem_med = np.array([])\n",
    "f2_lem_min = np.array([])\n",
    "f2_lem_max = np.array([])\n",
    "f3_lem_mean = np.array([])\n",
    "f3_lem_med = np.array([])\n",
    "f3_lem_min = np.array([])\n",
    "f3_lem_max = np.array([])\n",
    "\n",
    "for group in range(first_group, last_group + 1):\n",
    "    mask_gr = test_data.group_id == group\n",
    "    t_gr = test_data.title[mask_gr]\n",
    "    t_lem_gr = test_data.lemmatized_titles_with_nltk_and_maru[mask_gr]\n",
    "\n",
    "    cv_word_1_1 = CountVectorizer()\n",
    "    cv_word_1_1.fit(t_gr)\n",
    "    feat_1 = cv_word_1_1.transform(t_gr).toarray()\n",
    "    mean_1, med_1, min_1, max_1 = compute_jackard(feat_1)\n",
    "    \n",
    "    cv_lem_word_1_1 = CountVectorizer()\n",
    "    cv_lem_word_1_1.fit(t_lem_gr)\n",
    "    feat_lem_1 = cv_lem_word_1_1.transform(t_lem_gr).toarray()\n",
    "    mean_lem_1, med_lem_1, min_lem_1, max_lem_1 = compute_jackard(feat_lem_1)\n",
    "    \n",
    "    cv_word_2_2 = CountVectorizer(analyzer='word', ngram_range=(2,2))\n",
    "    cv_word_2_2.fit(t_gr)\n",
    "    feat_2 = cv_word_2_2.transform(t_gr).toarray()\n",
    "    mean_2, med_2, min_2, max_2 = compute_jackard(feat_2)\n",
    "    \n",
    "    cv_lem_word_2_2 = CountVectorizer()\n",
    "    cv_lem_word_2_2.fit(t_lem_gr)\n",
    "    feat_lem_2 = cv_lem_word_2_2.transform(t_lem_gr).toarray()\n",
    "    mean_lem_2, med_lem_2, min_lem_2, max_lem_2 = compute_jackard(feat_lem_2)\n",
    "    \n",
    "    cv_char_3_4 = CountVectorizer(analyzer='char', ngram_range=(3,4))\n",
    "    cv_char_3_4.fit(t_gr)\n",
    "    feat_3 = cv_char_3_4.transform(t_gr).toarray()\n",
    "    mean_3, med_3, min_3, max_3 = compute_jackard(feat_3)\n",
    "        \n",
    "    cv_lem_char_3_4 = CountVectorizer()\n",
    "    cv_lem_char_3_4.fit(t_lem_gr)\n",
    "    feat_lem_3 = cv_lem_char_3_4.transform(t_lem_gr).toarray()\n",
    "    mean_lem_3, med_lem_3, min_lem_3, max_lem_3 = compute_jackard(feat_lem_3)\n",
    "    \n",
    "    f1_mean = np.concatenate((f1_mean, mean_1))\n",
    "    f1_med = np.concatenate((f1_med, med_1))\n",
    "    f1_min = np.concatenate((f1_min, min_1))\n",
    "    f1_max = np.concatenate((f1_max, max_1))\n",
    "    f2_mean = np.concatenate((f2_mean, mean_2))\n",
    "    f2_med = np.concatenate((f2_med, med_2))\n",
    "    f2_min = np.concatenate((f2_min, min_2))\n",
    "    f2_max = np.concatenate((f2_max, max_2))\n",
    "    f3_mean = np.concatenate((f3_mean, mean_3))\n",
    "    f3_med = np.concatenate((f3_med, med_3))\n",
    "    f3_min = np.concatenate((f3_min, min_3))\n",
    "    f3_max = np.concatenate((f3_max, max_3))\n",
    "    \n",
    "    f1_lem_mean = np.concatenate((f1_lem_mean, mean_lem_1))\n",
    "    f1_lem_med = np.concatenate((f1_lem_med, med_lem_1))\n",
    "    f1_lem_min = np.concatenate((f1_lem_min, min_lem_1))\n",
    "    f1_lem_max = np.concatenate((f1_lem_max, max_lem_1))\n",
    "    f2_lem_mean = np.concatenate((f2_lem_mean, mean_lem_2))\n",
    "    f2_lem_med = np.concatenate((f2_lem_med, med_lem_2))\n",
    "    f2_lem_min = np.concatenate((f2_lem_min, min_lem_2))\n",
    "    f2_lem_max = np.concatenate((f2_lem_max, max_lem_2))\n",
    "    f3_lem_mean = np.concatenate((f3_lem_mean, mean_lem_3))\n",
    "    f3_lem_med = np.concatenate((f3_lem_med, med_lem_3))\n",
    "    f3_lem_min = np.concatenate((f3_lem_min, min_lem_3))\n",
    "    f3_lem_max = np.concatenate((f3_lem_max, max_lem_3))\n",
    "\n",
    "feat_to_df = {'mean_word_1_1': f1_mean, 'median_word_1_1': f1_med,\n",
    "              'min_word_1_1': f1_min, 'max_word_1_1': f1_max,\n",
    "              'mean_word_2_2': f2_mean, 'median_word_2_2': f2_med,\n",
    "              'min_word_2_2': f2_min, 'max_word_2_2': f2_max,\n",
    "              'mean_char_3_4': f3_mean, 'median_char_3_4': f3_med,\n",
    "              'min_char_3_4': f3_min, 'max_char_3_4': f3_max}\n",
    "df_features_test = pd.DataFrame(feat_to_df)\n",
    "\n",
    "feat_to_df = {'mean_word_1_1': f1_lem_mean, 'median_word_1_1': f1_lem_med,\n",
    "              'min_word_1_1': f1_lem_min, 'max_word_1_1': f1_lem_max,\n",
    "              'mean_word_2_2': f2_lem_mean, 'median_word_2_2': f2_lem_med,\n",
    "              'min_word_2_2': f2_lem_min, 'max_word_2_2': f2_lem_max,\n",
    "              'mean_char_3_4': f3_lem_mean, 'median_char_3_4': f3_lem_med,\n",
    "              'min_char_3_4': f3_lem_min, 'max_char_3_4': f3_lem_max}\n",
    "df_lem_features_test = pd.DataFrame(feat_to_df)\n",
    "\n",
    "print(df_lem_features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_word_1_1</th>\n",
       "      <th>median_word_1_1</th>\n",
       "      <th>min_word_1_1</th>\n",
       "      <th>max_word_1_1</th>\n",
       "      <th>mean_word_2_2</th>\n",
       "      <th>median_word_2_2</th>\n",
       "      <th>min_word_2_2</th>\n",
       "      <th>max_word_2_2</th>\n",
       "      <th>mean_char_3_4</th>\n",
       "      <th>median_char_3_4</th>\n",
       "      <th>min_char_3_4</th>\n",
       "      <th>max_char_3_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.938947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.938947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.938947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.982460</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982460</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982460</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.950288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.950288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.950288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.924959</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.924959</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.924959</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.967093</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967093</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967093</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_word_1_1  median_word_1_1  min_word_1_1  max_word_1_1  mean_word_2_2  \\\n",
       "0       0.938947              1.0      0.500000           1.0       0.938947   \n",
       "1       0.982460              1.0      0.857143           1.0       0.982460   \n",
       "2       0.950288              1.0      0.692308           1.0       0.950288   \n",
       "3       0.924959              1.0      0.200000           1.0       0.924959   \n",
       "4       0.967093              1.0      0.727273           1.0       0.967093   \n",
       "\n",
       "   median_word_2_2  min_word_2_2  max_word_2_2  mean_char_3_4  \\\n",
       "0              1.0      0.500000           1.0       0.938947   \n",
       "1              1.0      0.857143           1.0       0.982460   \n",
       "2              1.0      0.692308           1.0       0.950288   \n",
       "3              1.0      0.200000           1.0       0.924959   \n",
       "4              1.0      0.727273           1.0       0.967093   \n",
       "\n",
       "   median_char_3_4  min_char_3_4  max_char_3_4  \n",
       "0              1.0      0.500000           1.0  \n",
       "1              1.0      0.857143           1.0  \n",
       "2              1.0      0.692308           1.0  \n",
       "3              1.0      0.200000           1.0  \n",
       "4              1.0      0.727273           1.0  "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lem_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean_word_1_1      0\n",
       "median_word_1_1    0\n",
       "min_word_1_1       0\n",
       "max_word_1_1       0\n",
       "mean_word_2_2      0\n",
       "median_word_2_2    0\n",
       "min_word_2_2       0\n",
       "max_word_2_2       0\n",
       "mean_char_3_4      0\n",
       "median_char_3_4    0\n",
       "min_char_3_4       0\n",
       "max_char_3_4       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jackard_titles_test_not_nan.csv', mode='w', encoding='utf-8') as f_csv:\n",
    "#     df_features_test.fillna(1.0, inplace=True)\n",
    "    df_features_test.to_csv(f_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean_word_1_1      0\n",
       "median_word_1_1    0\n",
       "min_word_1_1       0\n",
       "max_word_1_1       0\n",
       "mean_word_2_2      0\n",
       "median_word_2_2    0\n",
       "min_word_2_2       0\n",
       "max_word_2_2       0\n",
       "mean_char_3_4      0\n",
       "median_char_3_4    0\n",
       "min_char_3_4       0\n",
       "max_char_3_4       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lem_features_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jackard_titles_lemmatization_test_not_nan.csv', mode='w', encoding='utf-8') as f_csv:\n",
    "#     df_lem_features_test.fillna(1.0, inplace=True)\n",
    "    df_lem_features_test.to_csv(f_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
