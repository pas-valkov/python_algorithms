{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "def batch_generator(X, y, shuffle=True, batch_size=1):\n",
    "    \"\"\"\n",
    "    Гератор новых батчей для обучения\n",
    "    X          - матрица объекты-признаки\n",
    "    y_batch    - вектор ответов\n",
    "    shuffle    - нужно ли случайно перемешивать выборку\n",
    "    batch_size - размер батча ( 1 это SGD, > 1 mini-batch GD)\n",
    "    Генерирует подвыборку для итерации спуска (X_batch, y_batch)\n",
    "    \"\"\"\n",
    "    size = X.shape[0]\n",
    "    if shuffle:\n",
    "        pos = np.random.permutation(np.arange(size))\n",
    "    else:\n",
    "        pos = np.arange(size)\n",
    "\n",
    "    for i in range(0, size - batch_size + 1, batch_size):\n",
    "        X_batch = X[pos[i: i + batch_size]]\n",
    "        y_batch = y[pos[i: i + batch_size]]\n",
    "        yield (X_batch, y_batch)\n",
    "\n",
    "# Теперь можно сделать генератор по данным ()\n",
    "#  my_batch_generator = batch_generator(X, y, shuffle=True, batch_size=1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Вычисляем значение сигмоида.\n",
    "    X - выход линейной модели\n",
    "    \"\"\"\n",
    "    sigm_value_x = 1 / (1 + np.exp(-x))\n",
    "    return sigm_value_x\n",
    "\n",
    "\n",
    "class MySGDClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, batch_generator, batch_size=1,\n",
    "                 C=1, alpha=0.01,\n",
    "                 max_epoch=10, model_type='lin_reg'):\n",
    "        \"\"\"\n",
    "        batch_generator -- функция генератор, которой будем создавать батчи\n",
    "        C - коэф. регуляризации\n",
    "        alpha - скорость спуска\n",
    "        max_epoch - максимальное количество эпох\n",
    "        model_type - тим модели, lin_reg или log_reg\n",
    "        \"\"\"\n",
    "\n",
    "        self.C = C\n",
    "        self.alpha = alpha\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_generator = batch_generator\n",
    "        self.errors_log = {'iter': [], 'loss': []}\n",
    "        self.model_type = model_type\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def calc_loss(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем функцию потерь по батчу\n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "        if self.model_type == \"lin_reg\":\n",
    "            dot = np.dot(X_batch, self.weights)\n",
    "            loss = np.sum((dot - y_batch) ** 2) / X_batch.shape[0]\n",
    "        elif self.model_type == \"log_reg\":\n",
    "            dot = sigmoid(np.dot(X_batch, self.weights))\n",
    "            temp1 = y_batch * np.log(dot) + (1 - y_batch) * np.log(1 - dot)\n",
    "            loss = -np.sum(temp1) / X_batch.shape[0]\n",
    "        loss += (np.sum(self.weights ** 2) / self.C)\n",
    "        return loss\n",
    "\n",
    "    def calc_loss_grad(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем градиент функции потерь по батчу (то что Вы вывели в задании 1)\n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "        if self.model_type == \"lin_reg\":\n",
    "            temp1 = np.dot(X_batch, self.weights)\n",
    "            loss_grad = 2 * (np.dot(temp1 - y_batch,\n",
    "                                    X_batch)) / X_batch.shape[0]\n",
    "        elif self.model_type == \"log_reg\":\n",
    "            temp1 = sigmoid(np.dot(X_batch, self.weights))\n",
    "            loss_grad = -np.dot(y_batch - temp1, X_batch) / X_batch.shape[0]\n",
    "        loss_grad += (2 * self.weights / self.C)\n",
    "        return loss_grad\n",
    "\n",
    "    def update_weights(self, new_grad):\n",
    "        \"\"\"\n",
    "        Обновляем вектор весов\n",
    "        new_grad - градиент по батчу\n",
    "        \"\"\"\n",
    "        self.weights -= self.alpha * new_grad\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Обучение модели\n",
    "        X - матрица объекты-признаки\n",
    "        y - вектор ответов\n",
    "        '''\n",
    "        X_upd = np.insert(X, 0, 1, axis=1)\n",
    "        self.weights = np.random.randn(X_upd.shape[1])\n",
    "        \n",
    "        for n in range(0, self.max_epoch):\n",
    "            new_epoch_generator = self.batch_generator(\n",
    "                X_upd, y, batch_size=self.batch_size)\n",
    "            \n",
    "            for batch_num, new_batch in enumerate(new_epoch_generator):\n",
    "                X_batch = new_batch[0]\n",
    "                y_batch = new_batch[1]\n",
    "                batch_grad = self.calc_loss_grad(X_batch, y_batch)\n",
    "                self.update_weights(batch_grad)\n",
    "\n",
    "                batch_loss = self.calc_loss(X_batch, y_batch)\n",
    "                self.errors_log['iter'].append(batch_num)\n",
    "                self.errors_log['loss'].append(batch_loss)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Предсказание класса\n",
    "        X - матрица объекты-признаки\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        '''\n",
    "        X_upd = np.insert(X, 0, 1, axis=1)\n",
    "        if self.model_type == \"lin_reg\":\n",
    "            y_hat = np.dot(X_upd, self.weights)\n",
    "        elif self.model_type == \"log_reg\":\n",
    "            y_hat = sigmoid(np.dot(X_upd, self.weights))\n",
    "        # Желательно здесь использовать матричные операции\n",
    "        # между X и весами, например, numpy.dot\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Боевое применение (3  балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import pymorphy2\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('russian')\n",
    "stemmer = nltk.stem.SnowballStemmer('russian')\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stop_words.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def title_preprocessing(title, stop_words, preprocessing_type='stemming'):\n",
    "    title = stop_words_remove(title.strip().split(), stop_words, preprocessing_type)\n",
    "    return ' '.join(title)\n",
    "\n",
    "def stop_words_remove(title_words, stop_words, preprocessing_type='stemming'):\n",
    "    #new_title = str()\n",
    "    new_title = []\n",
    "    for i in title_words:\n",
    "        i = i.strip(' !?@#$^&*\"\\'()_«»<>-+={}[]/\\.,:;') #%\n",
    "        if i in stop_words:\n",
    "            pass\n",
    "        else:\n",
    "            if preprocessing_type == 'stemming':\n",
    "                i = title_stemming(i)\n",
    "            else:\n",
    "                i = title_lemming(i)\n",
    "            if len(i) == 1:   # 11879 - problem\n",
    "                pass\n",
    "            else:\n",
    "                new_title.append(i)\n",
    "    return new_title\n",
    "\n",
    "def title_stemming(title_words):\n",
    "    return stemmer.stem(title_words)\n",
    "\n",
    "def title_lemming(title):\n",
    "    return morph.parse(title)[0].normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text = \"страница...\"\n",
    "text = text.strip('.')\n",
    "text = stemmer.stem(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026\n"
     ]
    }
   ],
   "source": [
    "# preprosecced_article = pd.read_csv('preprosecced_article.csv')\n",
    "\n",
    "doc_to_title = {}\n",
    "with open('docs_titles.tsv', encoding='utf-8') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            #title = data[1]\n",
    "            title = title_preprocessing(data[1], stop_words, preprocessing_type='lemming')\n",
    "#             title = preprosecced_article.loc[preprosecced_article['doc_id'] == doc_id, 'text'].values[0]\n",
    "        doc_to_title[doc_id] = title\n",
    "print(len(doc_to_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_info_from_csv(file, is_target):\n",
    "    train_data = pd.read_csv(file)\n",
    "    traingroups_titledata = {}\n",
    "    for i in range(len(train_data)):\n",
    "        new_doc = train_data.iloc[i]\n",
    "        doc_group = new_doc['group_id']\n",
    "        doc_id = new_doc['doc_id']\n",
    "        if is_target:\n",
    "            target = new_doc['target']\n",
    "        title = doc_to_title[doc_id]\n",
    "        if doc_group not in traingroups_titledata:\n",
    "            traingroups_titledata[doc_group] = []\n",
    "        if is_target:\n",
    "            traingroups_titledata[doc_group].append((doc_id, title, target))\n",
    "        else:\n",
    "            traingroups_titledata[doc_group].append((doc_id, title))\n",
    "    return train_data, traingroups_titledata\n",
    "\n",
    "\n",
    "#traingroups_titledata = extract_info_from_csv('train_groups.csv', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def create_x_y_groups_train(traingroups_titledata, is_target):\n",
    "    if is_target:\n",
    "        y_train = []\n",
    "    X_train = []\n",
    "    groups_train = []\n",
    "    for new_group in traingroups_titledata:\n",
    "        if is_target:\n",
    "            docs = traingroups_titledata[new_group]\n",
    "        else:\n",
    "            docs = list(map(lambda x: x + (0,), traingroups_titledata[new_group]))\n",
    "        for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "            if is_target:\n",
    "                y_train.append(target_id)\n",
    "            groups_train.append(new_group)\n",
    "            all_dist = []\n",
    "            words = set(title.strip().split())\n",
    "\n",
    "            for j in range(0, len(docs)):\n",
    "                if k == j:\n",
    "                    continue\n",
    "                \n",
    "                doc_id_j, title_j, target_j = docs[j]\n",
    "                words_j = set(title_j.strip().split())\n",
    "                all_dist.append(len(words.intersection(words_j)))\n",
    "            X_train.append(sorted(all_dist, reverse=True)[0:15])\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    groups_train = np.array(groups_train)\n",
    "    if is_target:\n",
    "        y_train = np.array(y_train)\n",
    "        print (X_train.shape, y_train.shape, groups_train.shape)\n",
    "        return X_train, y_train, groups_train\n",
    "    else:\n",
    "        print (X_train.shape, groups_train.shape)\n",
    "        return X_train, [], groups_train\n",
    "\n",
    "#X_train, y_train, groups_train = create_x_y_groups_train(traingroups_titledata, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram = 3\n",
    "\n",
    "def get_group_articles(data):\n",
    "    rez = []\n",
    "    for i in data:\n",
    "        rez.append(i)\n",
    "\n",
    "#     print(len(rez))\n",
    "\n",
    "    if len(rez) == 0:\n",
    "        rez.append('_'*ngram)\n",
    "    if len(rez) == 1:\n",
    "        if (len(rez[0]) < ngram):\n",
    "#             print(rez[0], sep='  ')\n",
    "            rez[0] += '_'*(ngram-len(rez[0]))\n",
    "    return rez\n",
    "\n",
    "\n",
    "def create_x_y_groups_train(traingroups_titledata, is_target):\n",
    "    if is_target:\n",
    "        y_train = []\n",
    "    X_train = []\n",
    "    groups_train = []\n",
    "    for new_group in traingroups_titledata:\n",
    "        if is_target:\n",
    "            docs = traingroups_titledata[new_group]\n",
    "        else:\n",
    "            docs = list(map(lambda x: x + (0,), traingroups_titledata[new_group]))\n",
    "        for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "            if is_target:\n",
    "                y_train.append(target_id)\n",
    "            groups_train.append(new_group)\n",
    "            all_dist = []\n",
    "            \n",
    "            vectorizer = CountVectorizer(analyzer='char', ngram_range=(ngram, ngram), stop_words=None)\n",
    "            try:\n",
    "                X1 = vectorizer.fit_transform((title.strip().split()))\n",
    "                words = set(vectorizer.get_feature_names())\n",
    "            except:\n",
    "                words = set(title.strip().split())\n",
    "            \n",
    "#             words = set(title.strip().split())\n",
    "\n",
    "            for j in range(0, len(docs)):\n",
    "                if k == j:\n",
    "                    continue\n",
    "                \n",
    "                doc_id_j, title_j, target_j = docs[j]\n",
    "                \n",
    "                vectorizer = CountVectorizer(analyzer='char', ngram_range=(3,3), stop_words=None)\n",
    "                try:\n",
    "                    X2 = vectorizer.fit_transform((title.strip().split()))\n",
    "                    words_j = set(vectorizer.get_feature_names())\n",
    "                except:\n",
    "                    words_j = set(title.strip().split())\n",
    "                \n",
    "#                 words_j = set(title_j.strip().split())\n",
    "                all_dist.append(len(words.intersection(words_j)))\n",
    "            X_train.append(sorted(all_dist, reverse=True)[0:15])\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    groups_train = np.array(groups_train)\n",
    "    if is_target:\n",
    "        y_train = np.array(y_train)\n",
    "        print (X_train.shape, y_train.shape, groups_train.shape)\n",
    "        return X_train, y_train, groups_train\n",
    "    else:\n",
    "        print (X_train.shape, groups_train.shape)\n",
    "        return X_train, [], groups_train\n",
    "\n",
    "#X_train, y_train, groups_train = create_x_y_groups_train(traingroups_titledata, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, traingroups_titledata = extract_info_from_csv('train_groups.csv', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, groups_train = create_x_y_groups_train(traingroups_titledata, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "traingroups_titledata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "def validation(X_train, y_train, groups_train, N, nn):\n",
    "    validation_size = int(len(set(groups_train)) / N)\n",
    "\n",
    "    val_arr = np.arange(len(set(groups_train)))+1\n",
    "\n",
    "    validation_group_numbers = val_arr[nn * validation_size:\n",
    "                                       (nn + 1) * validation_size]\n",
    "\n",
    "    X_test_validation = []\n",
    "    y_test_validation = []\n",
    "    X_train_validation = []\n",
    "    y_train_validation = []\n",
    "\n",
    "    for i_num, i in enumerate(groups_train):\n",
    "        if i in validation_group_numbers:\n",
    "            X_test_validation.append(X_train[i_num])\n",
    "            y_test_validation.append(y_train[i_num])\n",
    "        else:\n",
    "            X_train_validation.append(X_train[i_num])\n",
    "            y_train_validation.append(y_train[i_num])\n",
    "\n",
    "    X_test_validation = np.array(X_test_validation)\n",
    "    y_test_validation = np.array(y_test_validation)\n",
    "    X_train_validation = np.array(X_train_validation)\n",
    "    y_train_validation = np.array(y_train_validation)\n",
    "\n",
    "    return X_test_validation, y_test_validation, \\\n",
    "        X_train_validation, y_train_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "a_all = [0.3]\n",
    "c_all = [20]\n",
    "e_all = [100]\n",
    "# threshold = np.linspace(0.25, 0.35, 5)\n",
    "threshold = [0.35]\n",
    "N = 10\n",
    "\n",
    "best = 0.\n",
    "params = (0., 0., 0., \"\", 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3, 20, 100, 'log_reg', 0.35) 0.6526988659376126\n"
     ]
    }
   ],
   "source": [
    "params_all = itertools.product(a_all, c_all, e_all, threshold)\n",
    "\n",
    "for (a, c, e, th) in params_all:\n",
    "    sum_score = 0\n",
    "    for i in range(N):\n",
    "\n",
    "        X_test_validation, y_test_validation,\\\n",
    "            X_train_validation, y_train_validation = validation(X_train_norm,\n",
    "                                                                y_train,\n",
    "                                                                groups_train,\n",
    "                                                                N, i)\n",
    "\n",
    "        model = MySGDClassifier(batch_generator=batch_generator, alpha=a,\n",
    "                                C=c, max_epoch=e, model_type='log_reg',\n",
    "                                batch_size=X_train_validation.shape[0])\n",
    "        model.fit(X_train_validation, y_train_validation)\n",
    "\n",
    "        score = f1_score(y_test_validation,\n",
    "                         (model.predict(X_test_validation) > th).astype(int))\n",
    "        sum_score += score\n",
    "    sum_score /= N\n",
    "    if sum_score > best:\n",
    "        best = sum_score\n",
    "        params = (a, c, e, 'log_reg', th)\n",
    "print(params, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 15) (16627,)\n"
     ]
    }
   ],
   "source": [
    "test_data, traingroups_titledata = extract_info_from_csv('test_groups.csv', False)\n",
    "X_test, y_test, groups_test = create_x_y_groups_train(traingroups_titledata, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python37\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Program Files (x86)\\Python37\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Program Files (x86)\\Python37\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(np.concatenate((X_train, X_test)))\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С лучшими параметрами на валидации сделайте предсказание на тестовом множестве, отправьте его на проверку на платформу kaggle. Убедитесь, что Вы смогли побить public score первого бейзлайна. Если да, то Вы молодец!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3, 20, 100, 'log_reg', 0.35)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = MySGDClassifier(batch_generator=batch_generator, alpha=params[0],\n",
    "                             C=params[1], max_epoch=params[2],\n",
    "                             model_type=params[3],\n",
    "                             batch_size=X_train.shape[0])\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_predict = (best_model.predict(X_test) > params[4]).astype(int)\n",
    "rez = pd.DataFrame({'pair_id': test_data['pair_id'], 'target': y_predict})\n",
    "rez.to_csv(\"linlogreg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36891802489926023"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "402px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
